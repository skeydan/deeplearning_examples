{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one hidden layer with relU activation and squared error loss for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrap tensors in Variables\n",
    "# a Variable v is a node in a computational graph\n",
    "# v.data is a tensor\n",
    "# v.grad is another Variable holding the gradient of x wrt to some scalar\n",
    "# requires_grad=False: no need to compute gradients during the backward pass\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wrap weights in variables\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no need to keep references to intermediate values since we are not implementing the backward pass by hand\n",
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29532964.0\n"
     ]
    }
   ],
   "source": [
    "# loss is a Variable of shape (1,)\n",
    "# loss.data is a Tensor of shape (1,)\n",
    "# loss.data[0] is a scalar value holding the loss\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "       ...          â‹±          ...       \n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "[torch.FloatTensor of size 100x10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually zero the gradients before running the backward pass\n",
    "# will be needed when doing this in a loop\n",
    "w1.grad.data.zero_()\n",
    "w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use autograd to compute the complete backward pass\n",
    "# w1.data and w2.data will be Variables holding the gradient of the loss with respect to w1 and w2 respectively\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update weights using gradient descent\n",
    "# w1.data and w2.data are the weights \n",
    "# w1.grad.data and w2.grad.data are the gradients wrt to the weights\n",
    "w1.data -= learning_rate * w1.grad.data\n",
    "w2.data -= learning_rate * w2.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\t loss:21833930.0\n",
      "Iteration: 10\t loss:1697919.25\n",
      "Iteration: 20\t loss:171826.984375\n",
      "Iteration: 30\t loss:51356.10546875\n",
      "Iteration: 40\t loss:18955.466796875\n",
      "Iteration: 50\t loss:8050.6484375\n",
      "Iteration: 60\t loss:3793.65673828125\n",
      "Iteration: 70\t loss:1937.0438232421875\n",
      "Iteration: 80\t loss:1051.53466796875\n",
      "Iteration: 90\t loss:596.9180297851562\n",
      "Iteration: 100\t loss:350.2346496582031\n",
      "Iteration: 110\t loss:210.4900360107422\n",
      "Iteration: 120\t loss:128.79383850097656\n",
      "Iteration: 130\t loss:79.85877227783203\n",
      "Iteration: 140\t loss:50.01691436767578\n",
      "Iteration: 150\t loss:31.564390182495117\n",
      "Iteration: 160\t loss:20.0388240814209\n",
      "Iteration: 170\t loss:12.78122329711914\n",
      "Iteration: 180\t loss:8.183023452758789\n",
      "Iteration: 190\t loss:5.255309581756592\n",
      "Iteration: 200\t loss:3.383953332901001\n",
      "Iteration: 210\t loss:2.183619499206543\n",
      "Iteration: 220\t loss:1.4116019010543823\n",
      "Iteration: 230\t loss:0.9141182899475098\n",
      "Iteration: 240\t loss:0.5927556157112122\n",
      "Iteration: 250\t loss:0.3848434388637543\n",
      "Iteration: 260\t loss:0.2501632273197174\n",
      "Iteration: 270\t loss:0.16277633607387543\n",
      "Iteration: 280\t loss:0.10601932555437088\n",
      "Iteration: 290\t loss:0.06914503872394562\n",
      "Iteration: 300\t loss:0.04515349864959717\n",
      "Iteration: 310\t loss:0.029525605961680412\n",
      "Iteration: 320\t loss:0.019354281947016716\n",
      "Iteration: 330\t loss:0.012735527940094471\n",
      "Iteration: 340\t loss:0.008431704714894295\n",
      "Iteration: 350\t loss:0.00562283955514431\n",
      "Iteration: 360\t loss:0.0037932696286588907\n",
      "Iteration: 370\t loss:0.0025939918123185635\n",
      "Iteration: 380\t loss:0.0018049526261165738\n",
      "Iteration: 390\t loss:0.00127667305059731\n",
      "Iteration: 400\t loss:0.0009244810207746923\n",
      "Iteration: 410\t loss:0.0006826435565017164\n",
      "Iteration: 420\t loss:0.0005131640355102718\n",
      "Iteration: 430\t loss:0.0003935841959901154\n",
      "Iteration: 440\t loss:0.00030590922688134015\n",
      "Iteration: 450\t loss:0.00024267956905532628\n",
      "Iteration: 460\t loss:0.00019463321950752288\n",
      "Iteration: 470\t loss:0.00015949162479955703\n",
      "Iteration: 480\t loss:0.0001315823756158352\n",
      "Iteration: 490\t loss:0.00011023704428225756\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  if t % 10 == 0: print(\"Iteration: {}\\t loss:{}\".format(t, loss.data[0]))\n",
    "    \n",
    "  w1.grad.data.zero_()\n",
    "  w2.grad.data.zero_()\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  w1.data -= learning_rate * w1.grad.data\n",
    "  w2.data -= learning_rate * w2.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
