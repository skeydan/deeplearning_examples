{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network with PyTorch: nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one hidden layer with relU activation and squared error loss for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrap tensors in Variables\n",
    "# a Variable v is a node in a computational graph\n",
    "# v.data is a tensor\n",
    "# v.grad is another Variable holding the gradient of x wrt to some scalar\n",
    "# requires_grad=False: no need to compute gradients during the backward pass\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Module objects override the __call__ operator so you can call them like functions\n",
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = loss_fn(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation using autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From PyTorch documentation:\n",
    "\n",
    "Each Variable has a .creator attribute, that points to the function, of which it is an output.\n",
    "\n",
    "This is an entry point to a directed acyclic graph (DAG) consisting of Function objects as nodes, and references between them being the edges.\n",
    "\n",
    "Every time an operation is performed, a new Function representing it is instantiated, its forward() method is called, and its output Variable s creators are set to it. \n",
    "\n",
    "Then, by following the path from any Variable to the leaves, it is possible to reconstruct the sequence of operations that has created the data, and automatically compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the weights using gradient descent. \n",
    "for param in model.parameters():\n",
    "    param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 598.1449584960938\n",
      "10 338.0163269042969\n",
      "20 200.1707000732422\n",
      "30 112.33590698242188\n",
      "40 60.30597686767578\n",
      "50 31.86884307861328\n",
      "60 16.918045043945312\n",
      "70 9.090893745422363\n",
      "80 4.994498252868652\n",
      "90 2.8065693378448486\n",
      "100 1.6137747764587402\n",
      "110 0.9488677382469177\n",
      "120 0.5651736855506897\n",
      "130 0.3429165780544281\n",
      "140 0.2116415649652481\n",
      "150 0.13264800608158112\n",
      "160 0.08452332764863968\n",
      "170 0.054514262825250626\n",
      "180 0.03551381081342697\n",
      "190 0.02335045486688614\n",
      "200 0.015474722720682621\n",
      "210 0.01032930240035057\n",
      "220 0.006937050726264715\n",
      "230 0.004683402366936207\n",
      "240 0.0031785054598003626\n",
      "250 0.0021677673794329166\n",
      "260 0.0014843856915831566\n",
      "270 0.0010201847180724144\n",
      "280 0.0007036924944259226\n",
      "290 0.000486893201014027\n",
      "300 0.0003379075205884874\n",
      "310 0.00023514726490247995\n",
      "320 0.0001640560949454084\n",
      "330 0.00011472574988147244\n",
      "340 8.040101238293573e-05\n",
      "350 5.6457967730239034e-05\n",
      "360 3.971722617279738e-05\n",
      "370 2.7993079129373655e-05\n",
      "380 1.975927261810284e-05\n",
      "390 1.3967529412184376e-05\n",
      "400 9.888683962344658e-06\n",
      "410 7.009662567725172e-06\n",
      "420 4.974401235813275e-06\n",
      "430 3.5345462947589112e-06\n",
      "440 2.514363131922437e-06\n",
      "450 1.7899644717545016e-06\n",
      "460 1.2754857152685872e-06\n",
      "470 9.098421287490055e-07\n",
      "480 6.494190074590733e-07\n",
      "490 4.640430688596098e-07\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "  y_pred = model(x)\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  if t % 10 == 0: print(t, loss.data[0])\n",
    "  model.zero_grad()\n",
    "  loss.backward()\n",
    "  for param in model.parameters():\n",
    "    param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
