{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network with PyTorch: nn and PyTorch: optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one hidden layer with relU activation and squared error loss for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrap tensors in Variables\n",
    "# a Variable v is a node in a computational graph\n",
    "# v.data is a tensor\n",
    "# v.grad is another Variable holding the gradient of x wrt to some scalar\n",
    "# requires_grad=False: no need to compute gradients during the backward pass\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of the model for us.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Module objects override the __call__ operator so you can call them like functions\n",
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = loss_fn(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the weights using gradient descent. \n",
    "for param in model.parameters():\n",
    "    param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 637.9985961914062\n",
      "10 495.66107177734375\n",
      "20 387.77130126953125\n",
      "30 304.1816101074219\n",
      "40 236.8302764892578\n",
      "50 182.4066925048828\n",
      "60 138.5370330810547\n",
      "70 103.57554626464844\n",
      "80 76.1958999633789\n",
      "90 55.128326416015625\n",
      "100 39.23892593383789\n",
      "110 27.488317489624023\n",
      "120 18.942853927612305\n",
      "130 12.793649673461914\n",
      "140 8.467262268066406\n",
      "150 5.497020244598389\n",
      "160 3.5020952224731445\n",
      "170 2.175339698791504\n",
      "180 1.3183881044387817\n",
      "190 0.7838780879974365\n",
      "200 0.45691078901290894\n",
      "210 0.260831743478775\n",
      "220 0.14592903852462769\n",
      "230 0.08006779849529266\n",
      "240 0.04313898831605911\n",
      "250 0.022880928590893745\n",
      "260 0.011976495385169983\n",
      "270 0.006200934760272503\n",
      "280 0.0031864247284829617\n",
      "290 0.0016296543180942535\n",
      "300 0.000831937650218606\n",
      "310 0.0004248149925842881\n",
      "320 0.00021752533211838454\n",
      "330 0.00011191640805918723\n",
      "340 5.769787821918726e-05\n",
      "350 2.9720013117184862e-05\n",
      "360 1.5250707292580046e-05\n",
      "370 7.769182047923096e-06\n",
      "380 3.915505658369511e-06\n",
      "390 1.9464346223685425e-06\n",
      "400 9.527640827400319e-07\n",
      "410 4.5782073243572086e-07\n",
      "420 2.1572481045950553e-07\n",
      "430 9.959669711179231e-08\n",
      "440 4.5009869609202724e-08\n",
      "450 2.008169275313776e-08\n",
      "460 8.834404319202349e-09\n",
      "470 3.8923095857512635e-09\n",
      "480 1.7502522764445416e-09\n",
      "490 8.369919202166898e-10\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "  y_pred = model(x)\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  if t % 10 == 0: print(t, loss.data[0])\n",
    "  model.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
