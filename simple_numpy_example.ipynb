{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network with numpy only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one hidden layer with relU activation and squared error loss for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(N, D_in)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randn(N, D_out)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input -> hidden weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden -> output weights\n",
    "w2 = np.random.randn(H, D_out)\n",
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = x.dot(w1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_relu = np.maximum(h, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = h_relu.dot(w2)\n",
    "ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = np.square(ypred - y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient of the loss with respect to prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_loss_ypred = 2.0 * (ypred - y)\n",
    "grad_loss_ypred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient of loss with respect to w2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient of prediction wrt w2 is just h_relu\n",
    "grad_ypred_w2 = h_relu\n",
    "# gradient of loss with respect to w2 (chain rule)\n",
    "grad_loss_w2 = grad_ypred_w2.T.dot(grad_loss_ypred)\n",
    "grad_loss_w2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient of loss with respect to h_relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient of prediction wrt h_relu is just w2\n",
    "grad_ypred_hrelu = w2\n",
    "# gradient of loss with respect to  h_relu (chain rule)\n",
    "grad_loss_hrelu = grad_loss_ypred.dot(grad_ypred_hrelu.T)\n",
    "grad_loss_hrelu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient of loss with respect to h (pre-activation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set gradient to 0 if h<0 \n",
    "grad_loss_h = grad_loss_hrelu.copy()\n",
    "grad_loss_h[h < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient of loss with respect to w1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient of pre-activation wrt w1 is just input\n",
    "grad_h_w1 = x\n",
    "# gradient of loss with respect to w1 (chain rule)\n",
    "grad_loss_w1 = grad_h_w1.T.dot(grad_loss_h)\n",
    "grad_loss_w1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 -= learning_rate * grad_loss_w1\n",
    "w2 -= learning_rate * grad_loss_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\t loss:1.759570956100218e-20\n",
      "Iteration: 10\t loss:1.0625794096369956e-20\n",
      "Iteration: 20\t loss:6.579967656912601e-21\n",
      "Iteration: 30\t loss:4.193953250918141e-21\n",
      "Iteration: 40\t loss:2.7717575574265786e-21\n",
      "Iteration: 50\t loss:1.8972658167704706e-21\n",
      "Iteration: 60\t loss:1.3477257802523253e-21\n",
      "Iteration: 70\t loss:9.860068920803912e-22\n",
      "Iteration: 80\t loss:7.373461041192204e-22\n",
      "Iteration: 90\t loss:5.664133259862861e-22\n",
      "Iteration: 100\t loss:4.452531277218781e-22\n",
      "Iteration: 110\t loss:3.581374626682208e-22\n",
      "Iteration: 120\t loss:2.943461586389527e-22\n",
      "Iteration: 130\t loss:2.4514769288430213e-22\n",
      "Iteration: 140\t loss:2.0754561366015177e-22\n",
      "Iteration: 150\t loss:1.7764404564351458e-22\n",
      "Iteration: 160\t loss:1.549942659058507e-22\n",
      "Iteration: 170\t loss:1.3620412485312384e-22\n",
      "Iteration: 180\t loss:1.2104159678200468e-22\n",
      "Iteration: 190\t loss:1.0848448275990592e-22\n",
      "Iteration: 200\t loss:9.708521201586209e-23\n",
      "Iteration: 210\t loss:8.915654197952746e-23\n",
      "Iteration: 220\t loss:8.163680330612299e-23\n",
      "Iteration: 230\t loss:7.478842024658278e-23\n",
      "Iteration: 240\t loss:6.900104133315462e-23\n",
      "Iteration: 250\t loss:6.458162435210079e-23\n",
      "Iteration: 260\t loss:6.032708531942525e-23\n",
      "Iteration: 270\t loss:5.624996907346415e-23\n",
      "Iteration: 280\t loss:5.238493919141274e-23\n",
      "Iteration: 290\t loss:4.9125593444205027e-23\n",
      "Iteration: 300\t loss:4.6530554809524365e-23\n",
      "Iteration: 310\t loss:4.417183086966061e-23\n",
      "Iteration: 320\t loss:4.237901807638654e-23\n",
      "Iteration: 330\t loss:4.033850432927642e-23\n",
      "Iteration: 340\t loss:3.7772632280186e-23\n",
      "Iteration: 350\t loss:3.648104810536788e-23\n",
      "Iteration: 360\t loss:3.50449015154407e-23\n",
      "Iteration: 370\t loss:3.3205117453137734e-23\n",
      "Iteration: 380\t loss:3.154391813982723e-23\n",
      "Iteration: 390\t loss:3.0365772639947613e-23\n",
      "Iteration: 400\t loss:2.919289990395292e-23\n",
      "Iteration: 410\t loss:2.7841658788819664e-23\n",
      "Iteration: 420\t loss:2.7020318990493104e-23\n",
      "Iteration: 430\t loss:2.617062827317409e-23\n",
      "Iteration: 440\t loss:2.514724044440832e-23\n",
      "Iteration: 450\t loss:2.4351959239395955e-23\n",
      "Iteration: 460\t loss:2.3563102292882567e-23\n",
      "Iteration: 470\t loss:2.2674967999904567e-23\n",
      "Iteration: 480\t loss:2.173647201099684e-23\n",
      "Iteration: 490\t loss:2.1200081329498308e-23\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    ypred = h_relu.dot(w2)\n",
    "    loss = np.square(ypred - y).sum()\n",
    "    if t % 10 == 0: print(\"Iteration: {}\\t loss:{}\".format(t, loss))\n",
    "    grad_loss_ypred = 2.0 * (ypred - y)\n",
    "    grad_ypred_w2 = h_relu\n",
    "    grad_loss_w2 = grad_ypred_w2.T.dot(grad_loss_ypred)\n",
    "    grad_ypred_hrelu = w2\n",
    "    grad_loss_hrelu = grad_loss_ypred.dot(grad_ypred_hrelu.T)\n",
    "    grad_loss_h = grad_loss_hrelu.copy()\n",
    "    grad_loss_h[h < 0] = 0\n",
    "    grad_h_w1 = x\n",
    "    grad_loss_w1 = grad_h_w1.T.dot(grad_loss_h)\n",
    "    w1 -= learning_rate * grad_loss_w1\n",
    "    w2 -= learning_rate * grad_loss_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
